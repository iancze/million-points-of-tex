% \documentclass[twocolumn]{aastex62}
\documentclass[modern]{aastex62}

\usepackage{bm}
\usepackage{amsmath}


\newcommand{\radmc}{\texttt{RADMC-3D}}
\newcommand\kms{\ifmmode{\rm km\thinspace s^{-1}}\else km\thinspace s$^{-1}$\fi}
\newcommand{\todo}[1]{ \textcolor{red}{#1}}
\newcommand{\vt}{ {\bm \theta}}
\newcommand{\msun}{M$_\odot$}
\newcommand{\obj}{GW\,Ori}
\newcommand{\twelve}{${}^{12}$CO}
\newcommand{\thirteen}{${}^{13}$CO}
\newcommand{\eighteen}{C${}^{18}$O}

\newcommand{\vd}{\boldsymbol{\mathcal{V}}} % visibility data
\newcommand{\vm}{\boldsymbol{\mathcal{W}}} % visibility model
\newcommand{\bbeta}{\boldsymbol{\beta}} % disk structure parameter vector
\newcommand{\btheta}{\boldsymbol{\theta}} % all parameter vector

% standardizing notation
%
% disk parameters
% i_\mathrm{disk} (not i_d or i)
%
% orbit parameters
% i_\mathrm{in}
% i_\mathrm{out}

% stellar masses
% M_\ast

\begin{document}


\title{A Million Points of Light}

\correspondingauthor{Ian Czekala}
\email{iczekala@berkeley.edu}

\author[0000-0002-1483-8811]{Ian Czekala}
\altaffiliation{NASA Hubble Fellowship Program Sagan Fellow}
\affiliation{Department of Astronomy, 501 Campbell Hall, University of California, Berkeley, CA 94720-3411, USA}


\begin{abstract}
Fast computation of protoplanetary disk channel maps for molecular gas lines. With gradients (we hope).
\end{abstract}

\keywords{protoplanetary disks -- stars: pre-main sequence}

\section{Introduction} \label{sec:intro}

There have been some cool new techniques to model rotation curves derived from high resolution (spatial and spectra) images of protoplanetary disks. for example, \citet{yen16,yen18} stacking lines to detect faint transitions and measure dynamical masses. Also to probe perturbations to velocity fields to search for the influence of exoplanets \citet{teague18a} and perturbations to the pressure profiles in disks \citet{teague18c}.

In this paper, we try to incorporate some ideas inspired by these data-driven techniques into our modus operandi, which is fitting the visibilities directly using the Fourier transform of a sky brightness model. We'll also try to be smart about how we calculate the model visibilities, in such a way that we can also calculate derivatives in order to efficiently use high dimensional optimization and inference schemes.

Because our goal is to make the Fourier part somewhat generic to the parameterization of the sky model, we'll take the following order. First, we'll describe just enough of the sky model to give context to what we mean. Then, we'll develop the mathematical framework to calculate the model visibilities and their derivatives. Finally, we'll circle back and describe a specific implementation for the sky model and discuss how it might be extended.

% \begin{figure*}[ht!]
% \begin{center}
%   \includegraphics[width=\linewidth]{moments.pdf}
%   \figcaption{({\it left}) A 226\,GHz continuum image.  Contours start at 5$\times$ the RMS noise level and increase by factors of 2.  The synthesized beam geometry is shown in the lower left corner.  ({\it middle, left to right}) Maps of the $^{12}$CO, $^{13}$CO, and C$^{18}$O velocity-integrated intensities (contours, starting at 10, 3, and 3$\times$ the RMS noise levels, respectively, and increasing by factors of 2) overlaid on the intensity-weighted projected velocities (color-scale).  Note the prominent molecular cloud contamination in the $^{12}$CO map (see also Fig.~\ref{fig:chanmaps}).  ({\it right}) Spatially integrated spectra (inside the same {\tt CLEAN} mask, and smoothed with an 0.85\,km\,s$^{-1}$ Hanning kernel) for each CO line.
%   \label{fig:moments}}
%   \end{center}
% \end{figure*}

\section{Sky model basics}
Interferometers like ALMA measure the Fourier transform of the sky brightness, as a function of frequency. Our first task is to describe the sky brightness model, $I(x, y, \nu)$, which is a function of position and frequency. This has units of intensity, or something like Jy/beam or Jy/sterradian. The position $x$, $y$, is directly related to the RA, DEC, based upon the distance to the source, so we end up with a model of $I(l, m, \nu)$, where $l$ is the RA coordinate (in radians) and $m$ is the dec coordinate (in radians). Like RA, $l$ increases towards the east (left) of images.

The model parameters generally break down into two groups, those that are required to describe the morphology of the intensity, and those geometric parameters that describe the rotation or translation of the emission. The second group of parameters can be easily applied in the Fourier domain via the shift and rotation theorems. We'll lump the first group of parameters into $\bbeta$. The second group of parameters includes the RA ($\alpha$) and Dec ($\delta$) offsets, as well as the position angle of the ascending note $\Omega$. Together, we'll call all parameters $\btheta = \left \{ \bbeta, \alpha, \delta, \Omega \right \}$.

\section{Calculating visibilities}
The visibility function $V$ is simply the Fourier transform of the sky brightness distribution
\begin{equation}
  V(u, v, \nu) \rightleftharpoons I(\alpha, \delta, \nu)
\end{equation}
the $u$ and $v$ coordinates are the \emph{spatial frequencies} and have units of k$\lambda$. In this parameterization, $\alpha$ and $\delta$ are commonly denoted by $l$ and $m$ and have units of radians, rather than arcseconds. The visibility function is a complex valued function. So, going from a 2D ($\times N_\mathrm{chan}$) sky image cube to a corresponding visibility cube is pretty straightforward, and just involves using an FFT algorithm. For terminology's sake, we'll call the output of the FFT the \emph{dense visibility model}, or $\vm_\blacksquare$.

A typical measurement set from ALMA consists of discrete measurements of $V$ at many tens of thousands of $\{ u, v\}_i$ points. The exact values of the $u_i$ and $v_i$ will usually not correspond exactly to the grid points of the $u$ and $v$ values in the dense visibility model, and so some interpolation scheme is required to generate \emph{model visibilities} $V(u_i, v_i)$. Crucially, and perhaps obviously, the maximum $u$ and $v$ in the dense visibility model must be larger than the largest $\{ u, v\}_i$ in the measurement set.

\subsection{Interpolation schemes}
While in theory one could just go ahead with a nearest neighbor or linear interpolation scheme to go from $\vm_\blacksquare$ to $\vm$, this isn't the wisest thing to do. That's because any interpolation scheme with finite support will affect the signal that we have sampled. Sort of, e.g., we can't do full sinc interpolation. But, radio astronomers like to write down the interpolation as the convolution with a prolate spheroidal wavefunction, as best described in \citep{schwab84}. Modern packages use this implementation, and so we'll describe what happens.

First, the image is multiplied by a window function to taper support. We summarize the picture graphically in 1D. Then, the sampling occurs as the multiplication.

E.g., can we write the entire process that generates $\vm$ from $I$ as a series of matrix multiplications? I think so, it should be entirely linear.
\begin{equation}
  \vm = \boldsymbol{S} \mathcal{F} \{ T \times I\}
\end{equation}
where $\mathcal{F}$ represents the Fourier transform operator. Notably the taper window $T$ just premultiplies the image, this is not a matrix product.

Synthesis Imaging, chapter 16 has a good section.

\subsection{Transforming visibilities with theorems}
The position offsets from the phase center of the observations, $\alpha$ and $\delta$, transform the model visibilities ${\bm M}({\bm \theta})$ in straightforward, analytical ways. Rather than undergoing the full radiative transfer calculations to synthesize image plane models with these offsets, instead we can synthesize centered channel maps ($\delta_\alpha, \delta_\delta=0$) and use Fourier relationships to quickly modify the visibilities.
The two dimensional shift theorem says that an offset in the image plane results in a phase shift in the Fourier plane \citep{bracewell00}
\begin{equation}
I(\alpha - \delta_\alpha, \delta - \delta_\delta) \rightleftharpoons \exp \left \{ -2 \pi i (\delta_\alpha u + \delta_\delta v) \right \} {\bm M}(u,v).
\end{equation}
This means that every model visibility $M_i$ is phase-shifted by an amount that depends on its spatial frequencies $\{u_i, v_i\}$.
Since $\delta_\alpha$ and $\delta_\delta$ are ``nuisance parameters'' dependent on the calibration of the interferometric observations, we numerically marginalize the posterior probability distribution over them for each proposal of the remaining subset of parameters.
As before, let ${\bm \theta}$ equal the full set of parameters specified in \S\ref{subsec:interferometry}, and let
${\bm \theta}_r$ represent the subset of these parameters that excludes $\delta_\alpha$ and $\delta_\delta$. The  marginalized posterior distribution is given by the following double integral
\begin{equation}
p({\bm \theta}_r |\,{\bm D}) = \iint p({\bm \theta} |\,{\bm D}) \;\mathrm{d}\delta_\alpha\,\mathrm{d}\delta_\delta.
\label{eqn:integral}
\end{equation}
The range of this integral is set by the bounds of the prior distributions on $\delta_\alpha$ and $\delta_\delta$. From visual inspection of the channel maps of the dataset (e.g., see DM~Tau in Figure~\ref{fig:DMTau}), we can reliably determine the centroid of the disk emission to within 2\arcsec\ of the phase center. Therefore, we set flat ``tophat'' priors 2\arcsec\ in width centered on our estimates of the centroid. The two dimensional integral is performed via adaptive Gaussian cubature.\footnote{Implemented in \url{https://github.com/stevengj/HCubature.jl}, following \citet{genz80}}

If posterior distributions of $\delta_\alpha$ and $\delta_\delta$ are needed, for example to produce model and residual channel map figures as in Figure~\ref{fig:DMTau}, they can be easily approximated by the following procedure. From the collection of converged MCMC samples of $\{ {\bm \theta}_r\}$, randomly choose a single sample, denoted $\bar{{\bm \theta}_r}$. Using these parameters, initialize a disk structure, synthesize channel maps, and calculate model visibilities ${\bm M}(\bar{{\bm \theta}_r})$.
Use the same set of priors and likelihood function (Equation~\ref{eqn:likelihood}) to create a posterior probability function for $p(\delta_\alpha, \delta_\delta | {\bm D}, \bar{{\bm \theta}_r})$, which is functionally equivalent to the previous posterior distribution, but will only be sampled in $\delta_\alpha$ and $\delta_\delta$ \emph{conditional} on the value of $\bar{{\bm \theta}_r}$. This reduced posterior can be quickly explored via grid search or MCMC.
Although the resulting posteriors will not be exactly equivalent to those that would have been sampled as part of the full parameter space ${\bm \theta}$, given the usually tight constraints on ${\bm \theta}_r$, a single estimate of $\bar{{\bm \theta}_r}$ is sufficiently accurate for the purposes of generating model and residual channel maps. If a more accurate approximation is desired, additional choices of $\bar{{\bm \theta}_r}$ can be drawn from $\{ {\bm \theta}_r\}$ and the process repeated.

In principle, a similar marginalization procedure could also be applied to position angle (PA), since a rotation in the image plane is equivalent to a rotation in the Fourier plane \citep{bracewell00}. However, since this operation requires resampling the visibility array at the rotated spatial frequencies, it must be done using the full output from the FFT of the sky plane, rather than visibilities which have already been interpolated to the dataset baselines.
This requirement on the order of operations means that the marginalization over $\delta_\alpha$ and $\delta_\delta$ must occur at every step $\mathrm{d}\mathrm{PA}$ of the PA marginalization, making the numerical integration considerably more expensive. Since the computational cost of the Affine-Invariant MCMC algorithm (and of Metropolis-Hastings algorithms in general) scales linearly with dimensionality \citep{dunkley05,allison14}, to be worthwhile, an evaluation of the numerically marginalized posterior must take no more than $D/(D - 1)$ times longer than an evaluation of the original posterior, where $D$ is the dimensionality of the original problem. For the ray-tracing costs considered here, this holds true for $\delta_\alpha$ and $\delta_\delta$, but not for PA.


If the sky image is rotated by an angle PA such that the positions $\alpha$ and $\delta$ become
\begin{eqnarray}
\alpha^\prime = \alpha \cos \mathrm{PA} - \delta \sin \mathrm{PA} \\
\delta^\prime = \alpha \sin \mathrm{PA} + \delta \cos \mathrm{PA}
\end{eqnarray}
then its Fourier transform is rotated by the same angle and in the same sense \citep{bracewell00}
\begin{eqnarray}
u^\prime = u \cos \mathrm{PA} - v \sin \mathrm{PA} \\
v^\prime = u \sin \mathrm{PA} + v \cos \mathrm{PA}.
\end{eqnarray}
Since this operation requires resampling the visibility array at the rotated spatial frequencies, it must be done using the full output from the FFT of the sky plane, rather than visibilities which have already been interpolated to the dataset baselines.


\subsection{Likelihood Calculation}

Define our visibility data as $\boldsymbol{\mathcal{V}}$.
Define our visibility model as $\boldsymbol{\mathcal{W}}(\boldsymbol{\theta})$.

Typically done as
\begin{equation}
    {\cal L}(\boldsymbol{\theta}) = {\cal N}(\boldsymbol{\mathcal{V}} |\, \boldsymbol{\mathcal{W}}(\boldsymbol{\theta}), \boldsymbol{C})
\end{equation}
Typically we just treat the visibilities as independent measurements, and so we have a simple $\chi^2$ likelihood of
\begin{equation}
    \ln {\cal L}(\btheta) = - \frac{1}{2} \sum_i^N \frac{|\vd - \vm(\btheta)|^2}{\sigma_i^2}.
\end{equation}
Note that the likelihood is a scalar quantity. Next, we want to calculate the derivative of the likelihood
\begin{equation}
    \frac{\mathrm{d}\mathcal{L}(\btheta)}{\mathrm{d}\btheta}.
\end{equation}
The main image is $I(x,y,\nu)$. The 2D Fourier transform (FT) of this image is
\begin{equation}
    V(u,v,\nu) = \mathcal{F}(I).
\end{equation}
We're assuming that each frequency channel of the channel maps are independent. Baseline-dependent phase errors may make this less true. See \citet{hezaveh13} for the proper way to bake these into the inference loop, if we so desire, but we will just ignore them for now.

Real $\Re$ and imaginary $\Im$ and $\imath$.

DFM points out that, in theory, we can also create disk code that can deliver us a derivative of the likelihood function. This is great, because we can feed this into techniques like Hamiltonian Monte Carlo and access a much larger set of parameter space. We are going to need this, I think, if we actually want disk models that are realistic.

The idea is to start writing the change in derivative with respect to the interpolated visibilities.

Then, we probably need to write down the change in interpolated visibility. Due to the FFT, though, this is going to require a knowing the changes of all of the other pixels.

If we can reduce this to knowing the change in a single pixel, then that change in intensity, with respect to the parameters, is possible to compute.

It may be possible to introduce correction to the phase errors, as well.

\section{Modeling the radial intensity profile}
How smooth is the radial profile, and can we use a Nuker profile to model it?


\acknowledgments

% \software{CASA \citep[v4.4;][]{mcmullin07}, IRAF \citep{tody86,tody93}, DiskJockey \citep{czekala15a}, RADMC-3D \citep{dullemond12}, emcee \citep{foreman-mackey13}, VARTOOLS \citep{hartman16}, Astropy \citep{astropy13}}

\bibliographystyle{yahapj.bst}
\bibliography{biblio.bib}


\end{document}
